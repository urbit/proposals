---
type: post
date: ~2017.10.19..04.47.50..c107
title: UP 1 - Ford Caching Redux
author: ~rovnys-ricfer
navsort: bump
navuptwo: true
comments: reverse
---

## Metadata

```
  UP: 1
  Title: Ford Caching Redux
  Authors: ~rovnys-ricfer Ted Blackman <ted@tlon.io>
           ~pittyp-datfyn Anton Dyudin <anton@tlon.io>
  Created: ~2017.10.18
```

## Overview

### What Is Ford?

[Ford](https://urbit.org/docs/arvo/internals/ford) is Urbit's functional
reactive build system. It's one of the seven _vanes_ (kernel modules) inside
Arvo, the Urbit kernel. Conceptually, Ford is used to implement functions
from a path (`++beam`) to a built product.  Reactivity here means a client can
subscribe to changes to the output of a build, in case the build's dependencies
change. Ford is used in a variety of ways in Urbit, including the following:

- To compile Hoon source files into nock, including importing libraries (the
  compiler itself lives in hoon.hoon, not in Ford, but Ford wraps the
  Hoon parser (`++vast`) with a parser that also understands Ford runes used
  for importing libraries and structures)
- Validating and translating [marks](https://urbit.org/docs/arvo/marks)
- Ford runes are used to construct renderers: files that live in the `ren/`
  folder that respond to HTTP requests by assembling and transforming marked
  data from multiple files; renderers form the basis of the Tree website
  generation engine, among other uses
- The [Dojo](https://urbit.org/docs/using/shell) uses Ford to compile REPL expressions and interactive dialogs
- [Clay](https://urbit.org/docs/arvo/internals/clay) uses Ford to verify that the contents of a file match its mark

### What's Wrong with Ford?

What's wrong with Ford is primarily that webpages served by Urbit tend to respond with 504 errors. There's a cascade of reasons for these errors, presented
here from proximate to ultimate:

- The urbit takes too long to respond to a request for a static page
- The urbit is busy rebuilding the webpage when the request comes in, and it
  only processes one event at a time, so it doesn't even try to respond to the
  event until the build finishes
- The build might take longer than it needs to if the cache has been recently
  wiped, which is performed every few hours on a timer in lieu of an actual
  cache replacement algorithm, such as LRU or the clock algorithm
- The urbit likely doesn't even need to rebuild the current page, but it thinks
  it does because one of the dependencies has changed and it doesn't have a way
  to know that the change won't affect this build.

This problem is particularly pernicious on
[Fora](https://urbit.org/fora) because it updates relatively
frequently. The problem arises like this:

- A user is viewing the webpage for post A, which includes a sidebar with links
  to other Fora posts
- Someone posts a comment on post B; this *should* trigger a rebuild of only
  the webpage for post B
- All of the pages with the sidebar are rebuilt, but none of them change except
  for the post B page, since the sidebar depends only on the title of the post,
  not the contents or comments, and the title didn't change

The worst aspects of this situation can be solved by solving just these two
Problems:

- Ford doesn't track intermediate builds and their dependencies
- Ford doesn't have a real cache replacement algorithm

## Specification and Rationale

We propose a solution involving the following changes to Ford:

1. Modify Ford's dependency tracking to allow for dependencies on intermediate builds
2. Have Ford greedily rebuild when a dependency changes, before notifying subscribers of the changed build
3. Implement the "clock" cache replacement algorithm

### Intermediate Dependency Tracking

We need to change Ford's internal data structures to allow us to track
intermediate dependencies.  In present-day Ford, the dependencies of a build
are stored as a `(set beam)`, where `++beam` (defined in hoon.hoon) is a
referentially transparent path to a file at a particular location and revision
(in this case, referential transparency means a lookup of the file's contents
at that beam will always produce the same value, so you could replace a lookup
of the file's contents at that beam with a constant representing the file's
contents; hence, the reference is transparent). Each of these beams
represents a Clay path to which we're subscribed, so that we'll be notified
if there's a new version of a file at that path (or, if we've subscribed
to a folder, any change to the folder's contents).

To track intermediate dependencies, we need to be able to depend on more than
just a path: we need to be able to depend on the result of another build. We
propose changing the definition of a dependency from `++beam` to `++dent`:

```
++  dent                                                ::  individual dep
  $%  {$beam bem/beam ren/care}                         ::  dep on path
      {$boil bem/beam bom/beam arg/coin}                ::  dep on build
      {$load bem/beam mar/mark}                         ::  dep on mark
  ==
```

A `$beam` dent depends on a path in Clay, and the `ren/care` field (`++care`
is defined in zuse.hoon) determines our type of Clay subscription (file /
folder). A `$boil` dent represents the result of a build of a Hoon file and
associated dependencies -- an assembly called a "hood", with `++hood` defined
in zuse.hoon. The `arg/coin` field in the `$boil` dent is an argument for the
build, which includes the query string for HTTP requests. Finally, a `$load`
dent describes a beam that has undergone mark translation.

The dependency graph itself is stored in a `++nozzle`, which has two maps: one
from a dent to the set of its downstream dents (`sup`) and one from a dent to
the set of its upstream dents (`sub`). We store dependencies bidirectionally
because we need to traverse the graph from downstream to upstream when building
for the first time, and from upstream to downstream when rebuilding. `++nozzle`
operations are consolidated into the `++na` core for safety and consistency.
Ford's (singleton) cache now stores a `nozzle` in addition to the map from
hash (`@uvH`) to `++deps`.

```
++  cafe                                                ::  live cache
  $:  p/(set calx)                                      ::  used
      q/(map * calx)                                    ::  cache
      r/(map @uvH deps)                                 ::  hash to deps
      s/nozzle             ::  NEW                      ::  product to part
  ==                                                    ::
::
++  deps                                                ::  depend state
  %+  pair  (set dent)     ::  CHANGED FROM (set beam)  ::  dependencies
  $%  {$init $~}                                        ::  given out
      {$sent dux/(set duct)}                            ::  listener exists
      {$done $~}                                        ::  change seen
  ==                                                    ::
::
++  nozzle  {sub/(jug dent dent) sup/(jug dent dent)}   ::  bidirectional deps
::
++  na                                                  ::  nozzle operations
  |_  a/nozzle
  ++  put
    |=  {k/dent v/dent}  ^+  a
    [(~(put ju sup.a) k v) (~(put ju sub.a) v k)]
  ::
  ++  del
    |=  {k/dent v/dent}  ^+  a
    [(~(del ju sup.a) k v) (~(del ju sub.a) v k)]
  ::
  ++  add-sub                                           ::  k depends on dez
    |=  {k/dent dez/(set dent)}  ^+  a
    %+  roll  (~(tap in dez))
    =+  [v=*dent a=a]
    |.  (put(a a) k v)
  --
```

These data structures model a network graph of dependencies.  The `++nozzle`
maintains the keying from upstream (part) `dent`s to downstream (product)
`dent`s in its `sup` field, and it also maintains a reverse mapping from a
downstream product `dent` to its upstream part dependencies in its `sub` field.
When we first run a build (which consists of a number of nested build
operations), we need to traverse the dependency graph from outer build to inner
build as we go (calling `++add-sub:na`), meaning we traverse from downstream to
upstream.

But when a dependency (such as a Clay file) updates, we use that dependency
as a key to look up the set of downstream `dent`s that depend on that file
(note that multiple builds might depend on the same file, so a single file
changing could trigger multiple rebuilds). `++na` operations ensure the `sup`
and `sub` remain equivalent to each other. The `++add-sub:na` arm establishes
that a dent `k` depends on `dez`, a set of dents.

### Greedy Rebuilds

Present Ford will only rebuild when a client asks for a new build. We
propose changing that so Ford will rebuild a build "greedily" whenever that
build's dependencies change, anticipating a request for the new build. This
increases the latency with which a client is notified that their build is
stale, but should significantly decrease the latency in the response to a
request for the new build, because that request will only be made after the new
build is already complete. This approach should also integrate better with a
planned earthy caching system, since it reduces the number of cache
invalidation states from two (fresh, stale) to one (always fresh, as far as we
know).

It's also easier to implement cache promotion using greedy rebuilds. The
algorithm described in the following section assumes we're rebuilding greedily.

### Cache Promotion

In order to prevent Ford from unnecessarily re-running builds whose upstream dependencies haven't changed, we need to "promote" the cache from the previous 
cached revision of the dependencies to the new revision. This is a somewhat delicate
maneuver that we plan on doing using the following steps:

1. The contents of a Clay path that Ford is subscribed to changes, and
  Clay sends Ford a message notifying us of the change to that path. The
  message includes the old revision and new revision of the path.
2. Ford retrieves the latest version of the file from Clay.
3. Ford looks up the dependencies of the changed file in its state and
  rebuilds them.
4. Ford recurses to rebuild the dependencies immediately downstream of the
  last build, and checks whether all of that build's dependencies are the same
  as at the previous revision. If they are, it promotes the cached build from
  the previous revision to the new revision instead of re-running the build.
5. This recursion continues until the final build steps (the most downstream)
  are re-run. We then push notifications to subscribers that a new version of
  the build exists.\*

\* We also have plans for a change to the Ford API so that we'll only have
to notify subscribers if the result of the build is different. Even without
changing that, however, this should prevent us from running expensive
unnecessary rebuilds.

### Cache Replacement

We will implement the [clock cache replacement
algorithm](https://en.wikipedia.org/wiki/Page_replacement_algorithm#Clock) for
the Ford cache.  In order to do this, we need to set the number of
individual cache lines Ford can use. This number will be determined
empirically (our first guess is 4096).

We'll store a numeric index and "used" bit for each cache line, along with a
single number indicating the next index to check. Each time we place a build
in our cache, we'll run the following algorithm:

- Check if the cache line at the next index has its used bit set.
  - If the used bit is set, unset it and run this check at the next index (mod
    the total number of cache lines).
  - If the used bit is not set, replace the cache line at that index with the
    new cache line to be stored. Also remove the line from any other data
    structures that store it, to make sure its reference count drops to zero
    and its memory is freed.

Then, whenever we access a cached item, we set the "used" bit for its cache
line. This preferentially stores cache lines who are accessed more frequently.

#### Commentary

This is not perfect: some cache lines will take up significantly more memory
than others, which means we won't have a firm upper bound on the amount of
memory used by the Ford cache. We do hope that in practice, we should be
able to keep most build products cached while still maintaining a reasonable
safety margin.



## Integration Plan

This work can be fruitfully broken up into milestones:

1. Get Ford to compile using new deps data structures
2. Implement intermediate dependency tracking logic
3. Implement greedy rebuilding
4. Add cache replacement and remove the timer-based cache wiping

Each of these steps could potentially be deployed to the network individually.
