## `~rovnys-ricfer`
# Worklist-Based `+execute` Algorithm

## Motivation

Ford-turbo as written uses a recursive function `+execute` to perform builds, sub-builds, and client builds -- it recurses "upward" toward root builds, and downward toward leaf builds, such as `%scry` builds. This has gotten us through several test cases, but does not work in the case of a second request for a completed live build. It gets stuck after executing sub-builds, without recursing back up into the root.

Part of this issue is caused by the lack of an explicit data structure representing which builds are currently blocked on which other builds. More generally, though, the current implementation of `+execute` jumps around wildly and is hard to reason about.

Another issue with current `+execute` is that it is not parallelizable. While Vere does not yet support parallelism, we expect to build a multi-threaded version of `+turn` (Hoon's version of "map"). It would be great to be able to use `+p-turn` to parallelize Ford builds, which tend to be among the most computationally intense loads in Urbit (the other major CPU hog is the compiler, which might also benefit from this type of parallelism).

## Implementation

We can solve both of these problems by rearchitecting `+execute` to run `+make` batched on multiple builds, followed by a "reduce" step that aggregates the results of the `+make`s, and based on those results, mutates the persistent state, enqueues effects to be sent out at the end of the event, and produces a new batch of builds to execute. This should actually result in simpler code, since it will make the control flow more explicit and less chaotic.

There are multiple ways this could work. One extreme is that we keep running `+execute` over and over until the next batch is `~`, meaning we have completed all work. The other extreme is that after each batch, we stop and send a request to Behn to send us a new Arvo event -- node.js users might recognize this as the `setTimeout(0)` pattern. The advantage of the latter approach is that it minimizes blocking of the event loop, which will allow Urbit to process other events (such as keystrokes and HTTP requests) periodically during a protracted build. It might also increase parallelism, since new events could trigger new Ford builds, which would then be unified into the next batch.

The potential downside of the one-batch-per-event scheme is extra overhead from Arvo events, entering and leaving Ford, and also that a Ford build consisting of many small sub-builds will take as long to complete as any other concurrent build, even if it would have completed quickly during a single event.

Because we don't yet know how we'll want to tune this between the two extremes, we will implement the worklist-based `+execute` in such a way that the batch processing is decoupled from the per-event finalization step. This step produces the outgoing moves, most importantly the Clay subscriptions for changed files. By keedping this step separate, we could rejigger our event flow without a significant refactor.

## Integration

The first version will run `+execute` repeatedly until no more work can be done, fixing the problem with the recursion while leaving the opportunity to implement the parallelism in the future. This way we can also keep our test suite the same.
